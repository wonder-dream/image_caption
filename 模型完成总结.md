# ViT + Transformer å›¾åƒæè¿°æ¨¡å‹ - å®Œæˆæ€»ç»“

## âœ… å·²å®Œæˆçš„å·¥ä½œ

æˆ‘å·²ç»ä¸ºæ‚¨æˆåŠŸæ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ **Vision Transformer + Transformer Decoder** å›¾åƒæè¿°æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥è®­ç»ƒä½¿ç”¨ã€‚

---

## ğŸ“¦ åˆ›å»ºçš„æ ¸å¿ƒæ–‡ä»¶

### 1. æ¨¡å‹æ–‡ä»¶
- **`vit_transformer_model.py`** - å®Œæ•´çš„æ¨¡å‹å®ç°ï¼ˆçº¦470è¡Œï¼‰
  - `VisionTransformerEncoder` - ViTå›¾åƒç¼–ç å™¨ç±»
  - `TransformerDecoder` - Transformerè§£ç å™¨ç±»
  - `ViTTransformerCaptioning` - å®Œæ•´çš„å›¾åƒæè¿°æ¨¡å‹
  - `PositionalEncoding` - ä½ç½®ç¼–ç æ¨¡å—
  - æ”¯æŒGreedyæœç´¢å’ŒBeamæœç´¢ç”Ÿæˆ

### 2. è®­ç»ƒè„šæœ¬
- **`train_vit_transformer.py`** - å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ˆçº¦320è¡Œï¼‰
  - è‡ªåŠ¨æ•°æ®åŠ è½½
  - è®­ç»ƒå’ŒéªŒè¯å¾ªç¯
  - BLEU-4è¯„ä¼°
  - è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
  - TensorBoardæ”¯æŒ
  - å­¦ä¹ ç‡è°ƒåº¦
  - æ¸è¿›å¼è®­ç»ƒç­–ç•¥

### 3. æ¨ç†è„šæœ¬
- **`inference_vit_transformer.py`** - æ¨ç†å’Œæµ‹è¯•å·¥å…·ï¼ˆçº¦240è¡Œï¼‰
  - åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
  - å•å¼ /æ‰¹é‡å›¾åƒç”Ÿæˆ
  - éªŒè¯é›†è¯„ä¼°
  - å¯è§†åŒ–ç»“æœ
  - æ¯”è¾ƒä¸åŒç”Ÿæˆæ–¹æ³•

### 4. æµ‹è¯•è„šæœ¬
- **`test_model.py`** - å¿«é€Ÿæµ‹è¯•å·¥å…·ï¼ˆçº¦140è¡Œï¼‰
  - æµ‹è¯•æ¨¡å‹æ„å»º
  - æµ‹è¯•å‰å‘/åå‘ä¼ æ’­
  - æµ‹è¯•æ•°æ®åŠ è½½
  - æµ‹è¯•ä¼˜åŒ–å™¨
  - éªŒè¯æ¨¡å‹å¯è®­ç»ƒæ€§

### 5. æ–‡æ¡£
- **`README_VIT_TRANSFORMER.md`** - è¯¦ç»†ä½¿ç”¨æ–‡æ¡£
- **`ä½¿ç”¨æŒ‡å—.py`** - äº¤äº’å¼ä½¿ç”¨æŒ‡å—

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### å›¾åƒç¼–ç å™¨: Vision Transformer (ViT-B/16)
```
è¾“å…¥å›¾åƒ (224Ã—224Ã—3)
    â†“
Patchåˆ’åˆ† (14Ã—14 patches, æ¯ä¸ª16Ã—16)
    â†“
Linear Projection (patch_dim â†’ 768)
    â†“
Position Embedding
    â†“
Transformer Encoder (12å±‚)
    â†“
Projection (768 â†’ d_model)
    â†“
è¾“å‡ºç‰¹å¾ (batch_size, 196, d_model)
```

### æ–‡æœ¬è§£ç å™¨: Transformer Decoder
```
è¾“å…¥Caption (è¯IDåºåˆ—)
    â†“
Word Embedding (vocab_size â†’ d_model)
    â†“
Positional Encoding
    â†“
Transformer Decoder Layers (6å±‚)
    â”œâ”€ Self-Attention (å› æœmask)
    â”œâ”€ Cross-Attention (å…³æ³¨å›¾åƒç‰¹å¾)
    â””â”€ Feed-Forward Network
    â†“
Linear Layer (d_model â†’ vocab_size)
    â†“
è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
```

### æ¨¡å‹ç‰¹ç‚¹
- âœ… **çº¯Transformeræ¶æ„** - æ— CNNå’ŒRNN
- âœ… **é¢„è®­ç»ƒViT** - ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡
- âœ… **å¹¶è¡Œè®¡ç®—** - è®­ç»ƒé€Ÿåº¦å¿«
- âœ… **å¼ºå¤§çš„ç‰¹å¾** - ViTæ“…é•¿æ•è·å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯
- âœ… **çµæ´»ç”Ÿæˆ** - æ”¯æŒGreedyå’ŒBeamæœç´¢

---

## ğŸ“Š æ¨¡å‹é…ç½®ï¼ˆé»˜è®¤ï¼‰

```python
vocab_size = 109              # DeepFashionè¯å…¸å¤§å°
d_model = 512                 # Transformerç»´åº¦
nhead = 8                     # æ³¨æ„åŠ›å¤´æ•°
num_decoder_layers = 6        # è§£ç å™¨å±‚æ•°
dim_feedforward = 2048        # FFNç»´åº¦
dropout = 0.1                 # Dropoutç‡
max_len = 52                  # æœ€å¤§åºåˆ—é•¿åº¦
pretrained_vit = True         # ä½¿ç”¨é¢„è®­ç»ƒViT

batch_size = 32               # æ‰¹æ¬¡å¤§å°
learning_rate = 0.0001        # å­¦ä¹ ç‡
num_epochs = 30               # è®­ç»ƒè½®æ¬¡
```

**å‚æ•°é‡**: çº¦ **100M** (ä½¿ç”¨é»˜è®¤é…ç½®)

---

## ğŸš€ ä½¿ç”¨æµç¨‹

### ç¬¬1æ­¥: å®‰è£…ä¾èµ–

```bash
# å¿…éœ€
pip install torch torchvision

# å¯é€‰ï¼ˆç”¨äºè®­ç»ƒç›‘æ§å’Œè¯„ä¼°ï¼‰
pip install tensorboard nltk tqdm matplotlib pillow
```

### ç¬¬2æ­¥: æµ‹è¯•æ¨¡å‹

```bash
python test_model.py
```

éªŒè¯ï¼š
- âœ“ æ¨¡å‹æ„å»º
- âœ“ å‰å‘ä¼ æ’­
- âœ“ åå‘ä¼ æ’­
- âœ“ Captionç”Ÿæˆ
- âœ“ æ•°æ®åŠ è½½
- âœ“ ä¼˜åŒ–å™¨

### ç¬¬3æ­¥: å¼€å§‹è®­ç»ƒ

```bash
python train_vit_transformer.py
```

è®­ç»ƒè¿‡ç¨‹ï¼š
- Epoch 1-10: å†»ç»“ViTï¼Œåªè®­ç»ƒè§£ç å™¨
- Epoch 11-30: å¾®è°ƒæ•´ä¸ªæ¨¡å‹
- è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° `checkpoints/vit_transformer/best_model.pth`

### ç¬¬4æ­¥: ç›‘æ§è®­ç»ƒï¼ˆå¯é€‰ï¼‰

```bash
tensorboard --logdir=runs/vit_transformer
```

### ç¬¬5æ­¥: æ¨ç†æµ‹è¯•

```bash
python inference_vit_transformer.py
```

åŠŸèƒ½ï¼š
- åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
- åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
- æ¯”è¾ƒGreedyå’ŒBeamæœç´¢
- ç”Ÿæˆå¯è§†åŒ–ç»“æœ

---

## ğŸ¯ è®­ç»ƒç­–ç•¥

### é˜¶æ®µ1: é¢„è®­ç»ƒè§£ç å™¨ï¼ˆEpoch 1-10ï¼‰
- **ç›®æ ‡**: å¿«é€Ÿå­¦ä¹ è¯­è¨€æ¨¡å¼
- **ç­–ç•¥**: å†»ç»“ViTç¼–ç å™¨
- **å­¦ä¹ ç‡**: 1e-4
- **ä¼˜åŠ¿**: é¿å…ä¸€å¼€å§‹å°±ç ´åé¢„è®­ç»ƒçš„ViTç‰¹å¾

### é˜¶æ®µ2: ç«¯åˆ°ç«¯å¾®è°ƒï¼ˆEpoch 11-30ï¼‰
- **ç›®æ ‡**: é€‚åº”å…·ä½“ä»»åŠ¡
- **ç­–ç•¥**: è§£å†»ViTç¼–ç å™¨
- **å­¦ä¹ ç‡**: 
  - ç¼–ç å™¨: 1e-5 (å°å­¦ä¹ ç‡)
  - è§£ç å™¨: 1e-4
- **ä¼˜åŠ¿**: è®©è§†è§‰ç‰¹å¾é€‚åº”æè¿°ä»»åŠ¡

---

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### æ˜¾å­˜ä¸è¶³æ—¶
```python
config = {
    'batch_size': 16,           # å‡å°æ‰¹æ¬¡
    'd_model': 256,             # å‡å°ç»´åº¦
    'num_decoder_layers': 4,    # å‡å°‘å±‚æ•°
}
```

### è¿½æ±‚æ€§èƒ½æ—¶
```python
config = {
    'batch_size': 64,
    'd_model': 768,
    'num_decoder_layers': 8,
    'num_epochs': 50,
}
```

### å¿«é€Ÿæµ‹è¯•æ—¶
```python
config = {
    'pretrained_vit': False,    # ä¸åŠ è½½é¢„è®­ç»ƒ
    'num_epochs': 5,
    'batch_size': 16,
}
```

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

åœ¨DeepFashion-MultiModalæ•°æ®é›†ä¸Šï¼š
- **è®­ç»ƒæ—¶é—´**: 3-6å°æ—¶ï¼ˆGPUï¼Œ30 epochsï¼‰
- **BLEU-4**: 0.20-0.30ï¼ˆåˆç†èŒƒå›´ï¼‰
- **æœ€ä½³BLEU-4**: 0.30+ï¼ˆä¼˜ç§€ï¼‰

---

## ğŸ” ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”

| ç‰¹æ€§ | ViT + Transformer | CNN + RNN + Attention |
|------|-------------------|----------------------|
| **ç¼–ç å™¨** | Vision Transformer | ResNet/VGG |
| **è§£ç å™¨** | Transformer Decoder | LSTM/GRU + Attention |
| **å¹¶è¡Œæ€§** | âœ… é«˜ï¼ˆå…¨Transformerï¼‰ | âŒ ä½ï¼ˆRNNä¸²è¡Œï¼‰ |
| **è®­ç»ƒé€Ÿåº¦** | âœ… å¿« | âŒ æ…¢ |
| **é•¿è·ç¦»ä¾èµ–** | âœ… å¼º | âš ï¸ ä¸­ç­‰ |
| **å‚æ•°é‡** | âš ï¸ è¾ƒå¤§ï¼ˆ~100Mï¼‰ | âœ… ä¸­ç­‰ï¼ˆ~50Mï¼‰ |
| **é¢„è®­ç»ƒ** | âœ… ViTé¢„è®­ç»ƒ | âœ… CNNé¢„è®­ç»ƒ |
| **æ‰©å±•æ€§** | âœ… æ˜“æ‰©å±• | âš ï¸ RNNéš¾æ‰©å±• |

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
image_caption/
â”œâ”€â”€ data/                               # æ•°æ®é›†
â”‚   â”œâ”€â”€ images/                         # å›¾åƒæ–‡ä»¶
â”‚   â”œâ”€â”€ vocab.json                      # è¯å…¸ âœ…
â”‚   â”œâ”€â”€ train_data.json                 # è®­ç»ƒé›† âœ…
â”‚   â”œâ”€â”€ val_data.json                   # éªŒè¯é›† âœ…
â”‚   â””â”€â”€ test_data.json                  # æµ‹è¯•é›† âœ…
â”‚
â”œâ”€â”€ vit_transformer_model.py            # æ ¸å¿ƒæ¨¡å‹ âœ…
â”œâ”€â”€ train_vit_transformer.py            # è®­ç»ƒè„šæœ¬ âœ…
â”œâ”€â”€ inference_vit_transformer.py        # æ¨ç†è„šæœ¬ âœ…
â”œâ”€â”€ test_model.py                       # æµ‹è¯•è„šæœ¬ âœ…
â”‚
â”œâ”€â”€ deepfashion_dataset.py              # æ•°æ®é›†ç±»
â”œâ”€â”€ prepare_data.py                     # æ•°æ®é¢„å¤„ç†
â”‚
â”œâ”€â”€ README_VIT_TRANSFORMER.md           # è¯¦ç»†æ–‡æ¡£ âœ…
â”œâ”€â”€ ä½¿ç”¨æŒ‡å—.py                         # ä½¿ç”¨æŒ‡å— âœ…
â””â”€â”€ image_caption.ipynb                 # å‚è€ƒç¬”è®°æœ¬
```

---

## ğŸ“ æŠ€æœ¯äº®ç‚¹

### 1. å…ˆè¿›çš„æ¨¡å‹æ¶æ„
- ä½¿ç”¨æœ€æ–°çš„Vision Transformerä½œä¸ºç¼–ç å™¨
- çº¯Transformeræ¶æ„ï¼Œæ— RNNç“¶é¢ˆ
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ•è·å¤æ‚å…³ç³»

### 2. æ™ºèƒ½çš„è®­ç»ƒç­–ç•¥
- æ¸è¿›å¼è®­ç»ƒï¼ˆå…ˆè§£ç å™¨ï¼Œåç«¯åˆ°ç«¯ï¼‰
- å­¦ä¹ ç‡è°ƒåº¦ï¼ˆReduceLROnPlateauï¼‰
- æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
- æ—©åœæœºåˆ¶ï¼ˆä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰

### 3. çµæ´»çš„ç”Ÿæˆæ–¹æ³•
- **Greedyæœç´¢**: å¿«é€Ÿç”Ÿæˆ
- **Beamæœç´¢**: é«˜è´¨é‡ç”Ÿæˆ
- å¯è‡ªå®šä¹‰ç”Ÿæˆé•¿åº¦å’Œç­–ç•¥

### 4. å®Œå–„çš„å·¥å…·é“¾
- è®­ç»ƒç›‘æ§ï¼ˆTensorBoardï¼‰
- è‡ªåŠ¨è¯„ä¼°ï¼ˆBLEU-4ï¼‰
- æ¨ç†å·¥å…·ï¼ˆå¯è§†åŒ–ï¼‰
- æµ‹è¯•è„šæœ¬ï¼ˆéªŒè¯ï¼‰

---

## âœ¨ ä»£ç ç¤ºä¾‹

### æ¨¡å‹åˆå§‹åŒ–
```python
from vit_transformer_model import build_model

config = {
    'd_model': 512,
    'nhead': 8,
    'num_decoder_layers': 6,
}

model = build_model(vocab_size=109, config=config)
```

### è®­ç»ƒä¸€ä¸ªbatch
```python
images = ...  # (batch, 3, 224, 224)
captions = ...  # (batch, seq_len)

outputs = model(images, captions)
loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].view(-1))
loss.backward()
optimizer.step()
```

### ç”Ÿæˆcaption
```python
generated = model.generate(
    images,
    start_token=vocab['<start>'],
    end_token=vocab['<end>'],
    max_len=50,
    method='greedy'  # or 'beam_search'
)
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

### ç«‹å³å¯åš
1. âœ… å®‰è£…PyTorch: `pip install torch torchvision`
2. âœ… æµ‹è¯•æ¨¡å‹: `python test_model.py`
3. âœ… å¼€å§‹è®­ç»ƒ: `python train_vit_transformer.py`

### è¿›é˜¶æ”¹è¿›
1. æ•°æ®å¢å¼ºï¼ˆæ›´å¤šçš„å›¾åƒå˜æ¢ï¼‰
2. ä½¿ç”¨æ›´å¤§çš„ViTï¼ˆViT-L, ViT-Hï¼‰
3. å¤šæ¨¡æ€é¢„è®­ç»ƒï¼ˆCLIP, ALIGNï¼‰
4. å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆCIDErä¼˜åŒ–ï¼‰
5. é›†æˆå­¦ä¹ ï¼ˆå¤šæ¨¡å‹èåˆï¼‰

---

## ğŸ“ æ€»ç»“

âœ… **æ¨¡å‹**: Vision Transformer + Transformer Decoder  
âœ… **çŠ¶æ€**: å®Œæ•´å®ç°ï¼Œå¯ç›´æ¥è®­ç»ƒ  
âœ… **æ•°æ®**: å·²å‡†å¤‡ï¼ˆ25,636æ¡ï¼‰  
âœ… **æ–‡æ¡£**: è¯¦ç»†è¯´æ˜å’Œä½¿ç”¨æŒ‡å—  
âœ… **å·¥å…·**: è®­ç»ƒã€æ¨ç†ã€æµ‹è¯•è„šæœ¬é½å…¨  

**æ‚¨ç°åœ¨æ‹¥æœ‰ä¸€ä¸ªå®Œæ•´çš„ã€å¯è®­ç»ƒçš„ã€åŸºäºæœ€æ–°Transformeræ¶æ„çš„å›¾åƒæè¿°æ¨¡å‹ï¼**

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Vision Transformer**: "An Image is Worth 16x16 Words" (ICLR 2021)
2. **Transformer**: "Attention is All You Need" (NIPS 2017)
3. **Image Captioning**: "Show, Attend and Tell" (ICML 2015)

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥é˜… README_VIT_TRANSFORMER.md æˆ–è¿è¡Œ python ä½¿ç”¨æŒ‡å—.py**
